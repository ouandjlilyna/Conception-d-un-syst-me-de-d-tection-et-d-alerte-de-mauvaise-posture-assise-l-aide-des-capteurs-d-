# -*- coding: utf-8 -*-
"""classificiation_binaire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPm4eZWna26BfB2LxaQ0OpR4l-QxNcis
"""

import numpy as np 
import pandas as pd

# prepare env
!mkdir -p dataset/positive
!mkdir -p dataset/negative

import os
import glob
path_dataset_positive = os.getcwd() + (r'/dataset/positive')
path_dataset_negative = os.getcwd() + (r'/dataset/negative')

csv_files_positive = glob.glob(os.path.join(path_dataset_positive, "*.csv"))
csv_files_negative = glob.glob(os.path.join(path_dataset_negative, "*.csv"))


max_pos = []
min_pos = []
mean_pos = []
std_pos = []
var_pos = []
age_pos = []
gender_pos =[]

max_neg = []
min_neg = []
mean_neg = []
std_neg = []
var_neg = []
age_neg = []
gender_neg =[]

  
# loop over the list of csv files
def print_stat_data(directory, bool):
  for f in directory:

      df = pd.read_csv(f)
      max = df['data'].max()
      min = df['data'].min()
      mean = df['data'].mean()
      std = df['data'].std()
      var = df['data'].var()
      age = df['age'].values[0].astype(int)
      gender =df['gender'].values[0].astype(int)

      if bool:
        
        max_pos.append(max)
        min_pos.append(min)
        mean_pos.append(mean)
        std_pos.append(std)
        var_pos.append(var)
        age_pos.append(age)
        gender_pos.append(gender)
      else:
        
        max_neg.append(max)
        min_neg.append(min)
        mean_neg.append(mean)
        std_neg.append(std)
        var_neg.append(var)
        age_neg.append(age)
        gender_neg.append(gender)



    
      print(f"| max: {max}  , min: {min}  ,mean: {mean}  ,std: {std}  ,var: {var}  ,age: {age}  , gender: {gender} " )

print('---------------------------------------------------------------------------------------------------------------------------------------------')

print('  Bonne Posture\n')

print_stat_data(csv_files_positive, True)
print('---------------------------------------------------------------------------------------------------------------------------------------------')


print('  mauvaise Posture\n')

print_stat_data(csv_files_negative, False)

print('---------------------------------------------------------------------------------------------------------------------------------------------')

from sklearn.linear_model import SGDClassifier

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.preprocessing import scale

X = []
Y = []
for (max, min, mean, std, pos, age, gender) in zip(max_pos, min_pos, mean_pos, std_pos, var_pos, age_pos, gender_pos):
     X.append([max, min, mean, std, pos, age, gender])
     Y.append(0)

for (max, min, mean, std, pos, age, gender) in zip(max_neg, min_neg, mean_neg, std_neg, var_neg, age_neg, gender_neg):
     X.append([max, min, mean, std, pos, age, gender])
     Y.append(1)

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.20)

from sklearn.metrics import accuracy_score

from sklearn.svm import SVC


import numpy as np
import matplotlib.pyplot as plt
import time
svc = SVC() 
start = time.time()

kernel_list=['linear', 'poly', 'rbf', 'sigmoid']
train_accuracy = [] # Log training errors for each model
test_accuracy = [] # Log testing errors for each model

for x in kernel_list:
    svcc =SVC(kernel=x) 
    svcc.fit(xtrain,ytrain)
    train_z = svcc.predict(xtrain)
    test_z = svcc.predict(xtest)
    train_accuracy.append(accuracy_score(ytrain, train_z))
    test_accuracy.append(accuracy_score(ytest, test_z))

end = time.time()
runtime=end-start


print(f'Temps d\'exécution :  {runtime:.2} ms')
x = np.arange(len(kernel_list)) + 1 # Create domain for plot
plt.plot(x, train_accuracy, label='Training accuracy ') # Plot training accuracy
plt.plot(x, test_accuracy, label='Testing accuracy') # Plot testing accuracy
plt.xlabel('kernel: linear, poly, rbf, sigmoid') # Label x-axis
plt.ylabel('Total accuracy') # Label y-axis
plt.legend() # Show plot labels as legend
plt.show() # Show graph

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.svm import SVC
svc = SVC() 
start = time.time()
x='linear'
svcc =SVC(kernel=x) 
svcc.fit(xtrain,ytrain)
train_z = svcc.predict(xtrain)
test_z = svcc.predict(xtest)
test_accuracy=accuracy_score(ytest, test_z)
train_accuracy=accuracy_score(ytrain, train_z)
print(f'test accuracy :  {test_accuracy:.2} ') 
print(f'train accuracy :  {train_accuracy:.2} ') 

end = time.time()
runtime=end-start
y_pred = svcc.predict(xtest)
#print('Accuracy: {:.2f}'.format(accuracy_score(ytest, y_pred)))

#print(classification_report(ytest, y_pred))

print(f'Temps d\'exécution SVM :  {runtime:.2} ms')

print('Recall: %.3f' % recall_score(ytest, y_pred))
print('F1 Score: %.3f' % f1_score(ytest, y_pred))
print('Precision: %.3f' % precision_score(ytest, y_pred))

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.neighbors import KNeighborsClassifier
start = time.time()
    
k_list=range(1,11)
train_accuracy = [] 
test_accuracy = [] 

for x in k_list:
    knnn =KNeighborsClassifier(n_neighbors=x) 
    knnn.fit(xtrain,ytrain)
    train_z = knnn.predict(xtrain)
    test_z = knnn.predict(xtest)
    train_accuracy.append(accuracy_score(ytrain, train_z))
    test_accuracy.append(accuracy_score(ytest, test_z))

   
end = time.time()
runtime=end-start


print(f'Temps d\'exécution :  {runtime:.2} ms')




x = np.arange(len(k_list)) + 1 # Create domain for plot
plt.plot(x, train_accuracy, label='Training accuracy ') # Plot training accuracy over domain
plt.plot(x, test_accuracy, label='Testing accuracy ') # Plot testing accuracy over domain
plt.xlabel('Value of K for KNN') # Label x-axis
plt.ylabel('Total accuracy') # Label y-axis
plt.legend() # Show plot labels as legend
plt.show() # Show graph

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.neighbors import KNeighborsClassifier
start = time.time()
    
 
knnn=KNeighborsClassifier(n_neighbors=3) 
knnn.fit(xtrain,ytrain)
train_z = knnn.predict(xtrain)
test_z = knnn.predict(xtest)
test_accuracy= accuracy_score(ytrain, train_z)
train_accuracy= accuracy_score(ytest, test_z)

print(f'test accuracy :  {test_accuracy:.2} ') 
print(f'train accuracy :  {train_accuracy:.2} ') 
   
end = time.time()
runtime=end-start


print(f'Temps d\'exécution KNN :  {runtime:.2} ms')
print('Recall: %.3f' % recall_score(ytest, y_pred))
print('F1 Score: %.3f' % f1_score(ytest, y_pred))
print('Precision: %.3f' % precision_score(ytest, y_pred))

from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt
import time
max_depth_list = [1,2,3,4]

train_accuracy = [] 
test_accuracy = []  
start = time.time()
for x in max_depth_list:
    dtc = DecisionTreeClassifier(max_depth=x,random_state=0) 
    dtc.fit(xtrain,ytrain)
    train_z = dtc.predict(xtrain)
    test_z = dtc.predict(xtest)
    train_accuracy.append(accuracy_score(ytrain, train_z))
    test_accuracy.append(accuracy_score(ytest, test_z))
end = time.time()
runtime=end-start


print(f'Temps d\'exécution :  {runtime:.2} ms')

x = np.arange(len(max_depth_list)) + 1 # Create domain for plot
plt.plot(x, train_accuracy, label='Training Accuracy') 
plt.plot(x, test_accuracy, label='Testing Accuracy') 
plt.xlabel('Maximum Depth') # Label x-axis
plt.ylabel('Total accuracy') # Label y-axis
plt.legend() # Show plot labels as legend
plt.show() # Show graph

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
decision_tree = decision_tree.fit(xtrain, ytrain)
import time

start = time.time()
scrrrr = decision_tree.score(xtest, ytest)
print("Training score decision_tree : ", scrrrr)

y_predd = decision_tree.predict(xtrain)
y_pred = decision_tree.predict(xtest)
test_accuracy=accuracy_score(ytest, test_z)
train_accuracy=accuracy_score(ytrain, train_z)
print(' test Accuracy : {:.2f}'.format(accuracy_score(ytest, y_pred)))
print(' train Accuracy: {:.2f}'.format(accuracy_score(ytrain, y_predd)))

end = time.time()
runtime=end-start


print(f'Temps d\'exécution DT :  {runtime:.2} ms')

tree.plot_tree(decision_tree) 
print('Recall: %.3f' % recall_score(ytest, y_pred))
print('F1 Score: %.3f' % f1_score(ytest, y_pred))
print('Precision: %.3f' % precision_score(ytest, y_pred))

from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt
import time
start = time.time()

n_estimators_list= [1,2,3,4,5,6,7,8,9,10]
train_accuracy = [] 
test_accuracy = [] 

for x in n_estimators_list:
    rfc = RandomForestClassifier(n_estimators=x,random_state=0) 
    rfc.fit(xtrain,ytrain)
    train_z = rfc.predict(xtrain)
    test_z = rfc.predict(xtest)
    train_accuracy.append(accuracy_score(ytrain, train_z))
    test_accuracy.append(accuracy_score(ytest, test_z))

end = time.time()
runtime=end-start

print(f'Temps d\'exécution :  {runtime:.2} ms')

x = np.arange(len(n_estimators_list)) + 1 # Create domain for plot
plt.plot(x, train_accuracy, label='Training accuracy')
plt.plot(x, test_accuracy, label='Testing accuracy')
plt.xlabel(' number of trees in the forest') # Label x-axis
plt.ylabel('Total accuracy') # Label y-axis
plt.legend() # Show plot labels as legend
plt.show() # Show grap

from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt
import time
start = time.time()




rfc = RandomForestClassifier(n_estimators=1,random_state=0) 
rfc.fit(xtrain,ytrain)
train_z = rfc.predict(xtrain)
test_z = rfc.predict(xtest)
train_accuracy= accuracy_score(ytrain, train_z)
test_accuracy= accuracy_score(ytest, test_z)
print(f'test accuracy :  {test_accuracy:.2} ') 
print(f'train accuracy :  {train_accuracy:.2} ')    

end = time.time()
runtime=end-start

print(f'Temps d\'exécution :  {runtime:.2} ms')
print('Recall: %.3f' % recall_score(ytest, y_pred))
print('F1 Score: %.3f' % f1_score(ytest, y_pred))
print('Precision: %.3f' % precision_score(ytest, y_pred))

import pandas as pd

plotdata = pd.DataFrame(
    {"Training accuracy %": [89, 91, 91, 89,82],
    "Testing accuracy %": [91, 91, 91, 82,92] }, 
    index=["SVM", "KNN", "DT", "RF","MLP"])
# Plot a bar chart
plotdata.plot(kind="bar",title='Accuracy Comparison' )

from matplotlib import pyplot as plt


plt.ylabel('accuracy')
plt.xlabel('models')
plt.legend(['Train', 'Test'], loc='lower right')
plt.show()

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.2)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os, sys


def get_model_v1(shape):
  model = keras.models.Sequential()
  model.add(keras.layers.Input(shape,name="InputLayer"))
  model.add(keras.layers.Dense(10,activation='tanh', name= 'Dense_n1'))
  #model.add(keras.layers.Dense(30,activation='relu', name= 'Dense_n2'))
  #model.add(keras.layers.Dense(20,activation='sigmoid', name= 'Dense_n3'))
  #model.add(keras.layers.Dense(10,activation='swish', name= 'Dense_n4'))
  model.add(keras.layers.Dense(1,activation='sigmoid',name='Output'))
  model.compile(optimizer='adam' ,
              loss='binary_crossentropy' ,
              metrics= ['accuracy'] )
  return model

model= get_model_v1((7,))
#model.summary()
xtrain, ytrain = np.array(xtrain), np.array(ytrain)
xtest, ytest= np.array(xtest),np.array(ytest)
import time
start = time.time()


results= model.fit(xtrain,
                    ytrain,
                    epochs          =40,
                    batch_size      =1,
                   
                    validation_data = (xtest, ytest)
                   )
end = time.time()
runtime=end-start

print(f'Temps d\'exécution :  {runtime:.2} ms')
print("Test-Accuracy:", np.mean(results.history["val_accuracy"]))
print("Accuracy:", np.mean(results.history["accuracy"]))
print("loss:", np.mean(results.history["loss"]))

from matplotlib import pyplot as plt
plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='lower right')
plt.show()

plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

xtrain, ytrain = np.array(xtrain), np.array(ytrain)
xtest, ytest= np.array(xtest),np.array(ytest)

# predict probabilities for test set
yhat_probs = model.predict(xtest, verbose=0)
# predict crisp classes for test set
#yhat_classes = model.predict_classes(xtest, verbose=0)
yhat_classes = (model.predict(xtest) > 0.5).astype("int32")
# reduce to 1d array
yhat_probs = yhat_probs[:, 0]
yhat_classes = yhat_classes[:, 0]

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(ytest, yhat_classes)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(ytest, yhat_classes)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(ytest, yhat_classes)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(ytest, yhat_classes)
print('F1 score: %f' % f1)

import pickle
file_path = open('KNN_model', 'ab')
pickle.dump(knnn, file_path)
file_path.close()